{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 50.0,
  "eval_steps": 500,
  "global_step": 3150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.16,
      "grad_norm": 2.1895885467529297,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 2.9366,
      "step": 10
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.6566081047058105,
      "learning_rate": 6.031746031746032e-06,
      "loss": 3.0434,
      "step": 20
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.6742039918899536,
      "learning_rate": 9.206349206349207e-06,
      "loss": 2.9671,
      "step": 30
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.047008991241455,
      "learning_rate": 1.2380952380952381e-05,
      "loss": 2.7954,
      "step": 40
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.304673671722412,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 2.8818,
      "step": 50
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5518736839294434,
      "learning_rate": 1.8730158730158732e-05,
      "loss": 2.6779,
      "step": 60
    },
    {
      "epoch": 1.112,
      "grad_norm": 1.2757686376571655,
      "learning_rate": 2.1904761904761906e-05,
      "loss": 2.6526,
      "step": 70
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.393530249595642,
      "learning_rate": 2.507936507936508e-05,
      "loss": 2.5024,
      "step": 80
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.5912598371505737,
      "learning_rate": 2.8253968253968253e-05,
      "loss": 2.5628,
      "step": 90
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.2670087814331055,
      "learning_rate": 3.142857142857143e-05,
      "loss": 2.4474,
      "step": 100
    },
    {
      "epoch": 1.752,
      "grad_norm": 1.1888576745986938,
      "learning_rate": 3.4603174603174604e-05,
      "loss": 2.318,
      "step": 110
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.198823094367981,
      "learning_rate": 3.777777777777778e-05,
      "loss": 2.3954,
      "step": 120
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.415972352027893,
      "learning_rate": 4.095238095238095e-05,
      "loss": 2.324,
      "step": 130
    },
    {
      "epoch": 2.224,
      "grad_norm": 1.290328025817871,
      "learning_rate": 4.4126984126984126e-05,
      "loss": 2.224,
      "step": 140
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.9748206734657288,
      "learning_rate": 4.73015873015873e-05,
      "loss": 2.335,
      "step": 150
    },
    {
      "epoch": 2.544,
      "grad_norm": 1.2851043939590454,
      "learning_rate": 5.047619047619048e-05,
      "loss": 2.2388,
      "step": 160
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.6032605171203613,
      "learning_rate": 5.3650793650793654e-05,
      "loss": 2.2691,
      "step": 170
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.505102276802063,
      "learning_rate": 5.682539682539683e-05,
      "loss": 2.2673,
      "step": 180
    },
    {
      "epoch": 3.016,
      "grad_norm": 1.372385025024414,
      "learning_rate": 6e-05,
      "loss": 2.2093,
      "step": 190
    },
    {
      "epoch": 3.176,
      "grad_norm": 1.6155893802642822,
      "learning_rate": 6.317460317460318e-05,
      "loss": 2.2244,
      "step": 200
    },
    {
      "epoch": 3.336,
      "grad_norm": 1.5625338554382324,
      "learning_rate": 6.634920634920636e-05,
      "loss": 2.2092,
      "step": 210
    },
    {
      "epoch": 3.496,
      "grad_norm": 1.2702840566635132,
      "learning_rate": 6.952380952380952e-05,
      "loss": 2.1694,
      "step": 220
    },
    {
      "epoch": 3.656,
      "grad_norm": 1.3920536041259766,
      "learning_rate": 7.26984126984127e-05,
      "loss": 2.2923,
      "step": 230
    },
    {
      "epoch": 3.816,
      "grad_norm": 1.370261549949646,
      "learning_rate": 7.587301587301587e-05,
      "loss": 2.2162,
      "step": 240
    },
    {
      "epoch": 3.976,
      "grad_norm": 1.7031391859054565,
      "learning_rate": 7.904761904761905e-05,
      "loss": 2.1421,
      "step": 250
    },
    {
      "epoch": 4.128,
      "grad_norm": 1.3735064268112183,
      "learning_rate": 8.222222222222222e-05,
      "loss": 2.1997,
      "step": 260
    },
    {
      "epoch": 4.288,
      "grad_norm": 1.4734370708465576,
      "learning_rate": 8.53968253968254e-05,
      "loss": 2.1678,
      "step": 270
    },
    {
      "epoch": 4.448,
      "grad_norm": 1.236462116241455,
      "learning_rate": 8.857142857142857e-05,
      "loss": 2.1048,
      "step": 280
    },
    {
      "epoch": 4.608,
      "grad_norm": 1.15261709690094,
      "learning_rate": 9.174603174603175e-05,
      "loss": 2.2273,
      "step": 290
    },
    {
      "epoch": 4.768,
      "grad_norm": 1.5839917659759521,
      "learning_rate": 9.492063492063493e-05,
      "loss": 2.1485,
      "step": 300
    },
    {
      "epoch": 4.928,
      "grad_norm": 1.2146977186203003,
      "learning_rate": 9.80952380952381e-05,
      "loss": 2.1324,
      "step": 310
    },
    {
      "epoch": 5.08,
      "grad_norm": 1.3938175439834595,
      "learning_rate": 9.99995088061742e-05,
      "loss": 2.144,
      "step": 320
    },
    {
      "epoch": 5.24,
      "grad_norm": 1.4759235382080078,
      "learning_rate": 9.999398298646739e-05,
      "loss": 2.1447,
      "step": 330
    },
    {
      "epoch": 5.4,
      "grad_norm": 1.4437739849090576,
      "learning_rate": 9.998231803558968e-05,
      "loss": 2.0912,
      "step": 340
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 1.664978265762329,
      "learning_rate": 9.996451538596678e-05,
      "loss": 2.1243,
      "step": 350
    },
    {
      "epoch": 5.72,
      "grad_norm": 1.4602406024932861,
      "learning_rate": 9.994057722371777e-05,
      "loss": 2.138,
      "step": 360
    },
    {
      "epoch": 5.88,
      "grad_norm": 1.4946389198303223,
      "learning_rate": 9.991050648838675e-05,
      "loss": 2.1026,
      "step": 370
    },
    {
      "epoch": 6.032,
      "grad_norm": 1.2822191715240479,
      "learning_rate": 9.98743068725819e-05,
      "loss": 2.1,
      "step": 380
    },
    {
      "epoch": 6.192,
      "grad_norm": 1.232844352722168,
      "learning_rate": 9.983198282152188e-05,
      "loss": 2.0732,
      "step": 390
    },
    {
      "epoch": 6.352,
      "grad_norm": 1.4668116569519043,
      "learning_rate": 9.978353953249022e-05,
      "loss": 2.0891,
      "step": 400
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 1.1886096000671387,
      "learning_rate": 9.972898295419688e-05,
      "loss": 2.0588,
      "step": 410
    },
    {
      "epoch": 6.672,
      "grad_norm": 1.5414988994598389,
      "learning_rate": 9.966831978604788e-05,
      "loss": 2.0714,
      "step": 420
    },
    {
      "epoch": 6.832,
      "grad_norm": 1.6124382019042969,
      "learning_rate": 9.960155747732259e-05,
      "loss": 2.0637,
      "step": 430
    },
    {
      "epoch": 6.992,
      "grad_norm": 1.293636441230774,
      "learning_rate": 9.952870422625901e-05,
      "loss": 2.0775,
      "step": 440
    },
    {
      "epoch": 7.144,
      "grad_norm": 1.7390062808990479,
      "learning_rate": 9.944976897904701e-05,
      "loss": 2.0852,
      "step": 450
    },
    {
      "epoch": 7.304,
      "grad_norm": 1.2757885456085205,
      "learning_rate": 9.936476142872979e-05,
      "loss": 2.0651,
      "step": 460
    },
    {
      "epoch": 7.464,
      "grad_norm": 1.2006127834320068,
      "learning_rate": 9.927369201401358e-05,
      "loss": 2.0503,
      "step": 470
    },
    {
      "epoch": 7.624,
      "grad_norm": 1.2696999311447144,
      "learning_rate": 9.917657191798582e-05,
      "loss": 2.0448,
      "step": 480
    },
    {
      "epoch": 7.784,
      "grad_norm": 1.3201875686645508,
      "learning_rate": 9.907341306674185e-05,
      "loss": 2.0545,
      "step": 490
    },
    {
      "epoch": 7.944,
      "grad_norm": 1.3945484161376953,
      "learning_rate": 9.896422812792051e-05,
      "loss": 2.0258,
      "step": 500
    },
    {
      "epoch": 8.096,
      "grad_norm": 1.3572640419006348,
      "learning_rate": 9.884903050914849e-05,
      "loss": 2.0774,
      "step": 510
    },
    {
      "epoch": 8.256,
      "grad_norm": 1.423933982849121,
      "learning_rate": 9.872783435639397e-05,
      "loss": 1.9316,
      "step": 520
    },
    {
      "epoch": 8.416,
      "grad_norm": 1.2039899826049805,
      "learning_rate": 9.860065455222949e-05,
      "loss": 1.9709,
      "step": 530
    },
    {
      "epoch": 8.576,
      "grad_norm": 1.3909276723861694,
      "learning_rate": 9.846750671400446e-05,
      "loss": 2.0477,
      "step": 540
    },
    {
      "epoch": 8.736,
      "grad_norm": 1.5678064823150635,
      "learning_rate": 9.832840719192736e-05,
      "loss": 2.0538,
      "step": 550
    },
    {
      "epoch": 8.896,
      "grad_norm": 1.8299285173416138,
      "learning_rate": 9.818337306705796e-05,
      "loss": 2.0008,
      "step": 560
    },
    {
      "epoch": 9.048,
      "grad_norm": 1.6719627380371094,
      "learning_rate": 9.803242214920981e-05,
      "loss": 2.0562,
      "step": 570
    },
    {
      "epoch": 9.208,
      "grad_norm": 1.5055510997772217,
      "learning_rate": 9.78755729747633e-05,
      "loss": 2.0054,
      "step": 580
    },
    {
      "epoch": 9.368,
      "grad_norm": 1.3609020709991455,
      "learning_rate": 9.77128448043894e-05,
      "loss": 1.9771,
      "step": 590
    },
    {
      "epoch": 9.528,
      "grad_norm": 1.4793360233306885,
      "learning_rate": 9.754425762068447e-05,
      "loss": 2.0056,
      "step": 600
    },
    {
      "epoch": 9.688,
      "grad_norm": 1.301098108291626,
      "learning_rate": 9.736983212571646e-05,
      "loss": 1.9217,
      "step": 610
    },
    {
      "epoch": 9.848,
      "grad_norm": 1.4836931228637695,
      "learning_rate": 9.718958973848287e-05,
      "loss": 1.9877,
      "step": 620
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.0907461643218994,
      "learning_rate": 9.700355259228032e-05,
      "loss": 1.9805,
      "step": 630
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.488283634185791,
      "learning_rate": 9.681174353198687e-05,
      "loss": 1.9702,
      "step": 640
    },
    {
      "epoch": 10.32,
      "grad_norm": 1.4173918962478638,
      "learning_rate": 9.661418611125657e-05,
      "loss": 1.9659,
      "step": 650
    },
    {
      "epoch": 10.48,
      "grad_norm": 1.3975750207901,
      "learning_rate": 9.641090458962722e-05,
      "loss": 1.938,
      "step": 660
    },
    {
      "epoch": 10.64,
      "grad_norm": 1.6817066669464111,
      "learning_rate": 9.620192392954132e-05,
      "loss": 1.9725,
      "step": 670
    },
    {
      "epoch": 10.8,
      "grad_norm": 1.4883487224578857,
      "learning_rate": 9.598726979328079e-05,
      "loss": 1.9471,
      "step": 680
    },
    {
      "epoch": 10.96,
      "grad_norm": 1.5105619430541992,
      "learning_rate": 9.576696853981561e-05,
      "loss": 1.963,
      "step": 690
    },
    {
      "epoch": 11.112,
      "grad_norm": 1.277593731880188,
      "learning_rate": 9.554104722156716e-05,
      "loss": 1.9644,
      "step": 700
    },
    {
      "epoch": 11.272,
      "grad_norm": 1.8828133344650269,
      "learning_rate": 9.53095335810861e-05,
      "loss": 1.942,
      "step": 710
    },
    {
      "epoch": 11.432,
      "grad_norm": 1.5087438821792603,
      "learning_rate": 9.507245604764575e-05,
      "loss": 1.9171,
      "step": 720
    },
    {
      "epoch": 11.592,
      "grad_norm": 1.5532732009887695,
      "learning_rate": 9.482984373375105e-05,
      "loss": 1.9436,
      "step": 730
    },
    {
      "epoch": 11.752,
      "grad_norm": 1.5388309955596924,
      "learning_rate": 9.458172643156354e-05,
      "loss": 1.9131,
      "step": 740
    },
    {
      "epoch": 11.912,
      "grad_norm": 1.6417665481567383,
      "learning_rate": 9.432813460924307e-05,
      "loss": 1.9405,
      "step": 750
    },
    {
      "epoch": 12.064,
      "grad_norm": 1.530395746231079,
      "learning_rate": 9.40690994072063e-05,
      "loss": 1.9459,
      "step": 760
    },
    {
      "epoch": 12.224,
      "grad_norm": 1.649593472480774,
      "learning_rate": 9.380465263430276e-05,
      "loss": 1.8698,
      "step": 770
    },
    {
      "epoch": 12.384,
      "grad_norm": 1.658063530921936,
      "learning_rate": 9.353482676390887e-05,
      "loss": 1.9407,
      "step": 780
    },
    {
      "epoch": 12.544,
      "grad_norm": 2.0479514598846436,
      "learning_rate": 9.325965492994018e-05,
      "loss": 1.9029,
      "step": 790
    },
    {
      "epoch": 12.704,
      "grad_norm": 1.8468985557556152,
      "learning_rate": 9.297917092278271e-05,
      "loss": 1.9552,
      "step": 800
    },
    {
      "epoch": 12.864,
      "grad_norm": 1.8088608980178833,
      "learning_rate": 9.269340918514353e-05,
      "loss": 1.9018,
      "step": 810
    },
    {
      "epoch": 13.016,
      "grad_norm": 1.7721062898635864,
      "learning_rate": 9.24024048078213e-05,
      "loss": 1.9315,
      "step": 820
    },
    {
      "epoch": 13.176,
      "grad_norm": 1.5914251804351807,
      "learning_rate": 9.21061935253972e-05,
      "loss": 1.8648,
      "step": 830
    },
    {
      "epoch": 13.336,
      "grad_norm": 1.938170313835144,
      "learning_rate": 9.180481171184682e-05,
      "loss": 1.8812,
      "step": 840
    },
    {
      "epoch": 13.496,
      "grad_norm": 1.461674451828003,
      "learning_rate": 9.149829637607353e-05,
      "loss": 1.9283,
      "step": 850
    },
    {
      "epoch": 13.656,
      "grad_norm": 1.626041054725647,
      "learning_rate": 9.118668515736393e-05,
      "loss": 1.9082,
      "step": 860
    },
    {
      "epoch": 13.816,
      "grad_norm": 1.5999579429626465,
      "learning_rate": 9.087001632076573e-05,
      "loss": 1.8815,
      "step": 870
    },
    {
      "epoch": 13.975999999999999,
      "grad_norm": 1.6029821634292603,
      "learning_rate": 9.054832875238903e-05,
      "loss": 1.89,
      "step": 880
    },
    {
      "epoch": 14.128,
      "grad_norm": 1.7972142696380615,
      "learning_rate": 9.022166195463111e-05,
      "loss": 1.8633,
      "step": 890
    },
    {
      "epoch": 14.288,
      "grad_norm": 1.637499451637268,
      "learning_rate": 8.98900560413257e-05,
      "loss": 1.9127,
      "step": 900
    },
    {
      "epoch": 14.448,
      "grad_norm": 2.1951401233673096,
      "learning_rate": 8.955355173281708e-05,
      "loss": 1.8987,
      "step": 910
    },
    {
      "epoch": 14.608,
      "grad_norm": 2.0119481086730957,
      "learning_rate": 8.921219035095968e-05,
      "loss": 1.8329,
      "step": 920
    },
    {
      "epoch": 14.768,
      "grad_norm": 1.751603603363037,
      "learning_rate": 8.886601381404402e-05,
      "loss": 1.8795,
      "step": 930
    },
    {
      "epoch": 14.928,
      "grad_norm": 1.8186686038970947,
      "learning_rate": 8.851506463164907e-05,
      "loss": 1.8598,
      "step": 940
    },
    {
      "epoch": 15.08,
      "grad_norm": 2.0784037113189697,
      "learning_rate": 8.815938589942223e-05,
      "loss": 1.833,
      "step": 950
    },
    {
      "epoch": 15.24,
      "grad_norm": 1.8870184421539307,
      "learning_rate": 8.77990212937874e-05,
      "loss": 1.8488,
      "step": 960
    },
    {
      "epoch": 15.4,
      "grad_norm": 1.9682564735412598,
      "learning_rate": 8.743401506658152e-05,
      "loss": 1.8879,
      "step": 970
    },
    {
      "epoch": 15.56,
      "grad_norm": 1.6373343467712402,
      "learning_rate": 8.70644120396206e-05,
      "loss": 1.8298,
      "step": 980
    },
    {
      "epoch": 15.72,
      "grad_norm": 1.5246808528900146,
      "learning_rate": 8.66902575991957e-05,
      "loss": 1.8871,
      "step": 990
    },
    {
      "epoch": 15.88,
      "grad_norm": 1.5928587913513184,
      "learning_rate": 8.631159769049965e-05,
      "loss": 1.8379,
      "step": 1000
    },
    {
      "epoch": 16.032,
      "grad_norm": 1.8041951656341553,
      "learning_rate": 8.592847881198506e-05,
      "loss": 1.8431,
      "step": 1010
    },
    {
      "epoch": 16.192,
      "grad_norm": 1.8812320232391357,
      "learning_rate": 8.554094800965446e-05,
      "loss": 1.8015,
      "step": 1020
    },
    {
      "epoch": 16.352,
      "grad_norm": 2.723417043685913,
      "learning_rate": 8.51490528712831e-05,
      "loss": 1.8152,
      "step": 1030
    },
    {
      "epoch": 16.512,
      "grad_norm": 1.702662706375122,
      "learning_rate": 8.475284152057541e-05,
      "loss": 1.8429,
      "step": 1040
    },
    {
      "epoch": 16.672,
      "grad_norm": 1.978297233581543,
      "learning_rate": 8.435236261125546e-05,
      "loss": 1.8496,
      "step": 1050
    },
    {
      "epoch": 16.832,
      "grad_norm": 2.063727617263794,
      "learning_rate": 8.394766532109242e-05,
      "loss": 1.8606,
      "step": 1060
    },
    {
      "epoch": 16.992,
      "grad_norm": 1.4658668041229248,
      "learning_rate": 8.35387993458617e-05,
      "loss": 1.8295,
      "step": 1070
    },
    {
      "epoch": 17.144,
      "grad_norm": 1.5911659002304077,
      "learning_rate": 8.312581489324234e-05,
      "loss": 1.772,
      "step": 1080
    },
    {
      "epoch": 17.304,
      "grad_norm": 2.385683536529541,
      "learning_rate": 8.270876267665173e-05,
      "loss": 1.7773,
      "step": 1090
    },
    {
      "epoch": 17.464,
      "grad_norm": 1.699094295501709,
      "learning_rate": 8.228769390901812e-05,
      "loss": 1.7859,
      "step": 1100
    },
    {
      "epoch": 17.624,
      "grad_norm": 1.9584165811538696,
      "learning_rate": 8.186266029649174e-05,
      "loss": 1.8576,
      "step": 1110
    },
    {
      "epoch": 17.784,
      "grad_norm": 1.7334401607513428,
      "learning_rate": 8.143371403209554e-05,
      "loss": 1.8212,
      "step": 1120
    },
    {
      "epoch": 17.944,
      "grad_norm": 2.620332717895508,
      "learning_rate": 8.100090778931589e-05,
      "loss": 1.8045,
      "step": 1130
    },
    {
      "epoch": 18.096,
      "grad_norm": 2.5552642345428467,
      "learning_rate": 8.056429471563449e-05,
      "loss": 1.8126,
      "step": 1140
    },
    {
      "epoch": 18.256,
      "grad_norm": 1.732546329498291,
      "learning_rate": 8.012392842600198e-05,
      "loss": 1.7594,
      "step": 1150
    },
    {
      "epoch": 18.416,
      "grad_norm": 1.9431520700454712,
      "learning_rate": 7.967986299625418e-05,
      "loss": 1.7855,
      "step": 1160
    },
    {
      "epoch": 18.576,
      "grad_norm": 2.4036388397216797,
      "learning_rate": 7.923215295647166e-05,
      "loss": 1.8335,
      "step": 1170
    },
    {
      "epoch": 18.736,
      "grad_norm": 1.7094218730926514,
      "learning_rate": 7.878085328428369e-05,
      "loss": 1.8032,
      "step": 1180
    },
    {
      "epoch": 18.896,
      "grad_norm": 2.13039231300354,
      "learning_rate": 7.8326019398117e-05,
      "loss": 1.7955,
      "step": 1190
    },
    {
      "epoch": 19.048,
      "grad_norm": 2.1392862796783447,
      "learning_rate": 7.786770715039069e-05,
      "loss": 1.7418,
      "step": 1200
    },
    {
      "epoch": 19.208,
      "grad_norm": 1.8276010751724243,
      "learning_rate": 7.740597282065756e-05,
      "loss": 1.7966,
      "step": 1210
    },
    {
      "epoch": 19.368,
      "grad_norm": 1.7117313146591187,
      "learning_rate": 7.69408731086932e-05,
      "loss": 1.741,
      "step": 1220
    },
    {
      "epoch": 19.528,
      "grad_norm": 2.2472901344299316,
      "learning_rate": 7.64724651275334e-05,
      "loss": 1.7959,
      "step": 1230
    },
    {
      "epoch": 19.688,
      "grad_norm": 1.9111722707748413,
      "learning_rate": 7.600080639646077e-05,
      "loss": 1.7609,
      "step": 1240
    },
    {
      "epoch": 19.848,
      "grad_norm": 1.8946231603622437,
      "learning_rate": 7.552595483394164e-05,
      "loss": 1.7786,
      "step": 1250
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.1190991401672363,
      "learning_rate": 7.504796875051367e-05,
      "loss": 1.7591,
      "step": 1260
    },
    {
      "epoch": 20.16,
      "grad_norm": 1.603083610534668,
      "learning_rate": 7.456690684162557e-05,
      "loss": 1.7599,
      "step": 1270
    },
    {
      "epoch": 20.32,
      "grad_norm": 1.8416633605957031,
      "learning_rate": 7.408282818042941e-05,
      "loss": 1.7925,
      "step": 1280
    },
    {
      "epoch": 20.48,
      "grad_norm": 2.0692873001098633,
      "learning_rate": 7.359579221052665e-05,
      "loss": 1.7181,
      "step": 1290
    },
    {
      "epoch": 20.64,
      "grad_norm": 2.232539653778076,
      "learning_rate": 7.310585873866848e-05,
      "loss": 1.7626,
      "step": 1300
    },
    {
      "epoch": 20.8,
      "grad_norm": 2.011137008666992,
      "learning_rate": 7.261308792741188e-05,
      "loss": 1.7417,
      "step": 1310
    },
    {
      "epoch": 20.96,
      "grad_norm": 2.058110237121582,
      "learning_rate": 7.211754028773171e-05,
      "loss": 1.7512,
      "step": 1320
    },
    {
      "epoch": 21.112,
      "grad_norm": 2.025547981262207,
      "learning_rate": 7.161927667159013e-05,
      "loss": 1.7287,
      "step": 1330
    },
    {
      "epoch": 21.272,
      "grad_norm": 1.9182013273239136,
      "learning_rate": 7.111835826446416e-05,
      "loss": 1.7019,
      "step": 1340
    },
    {
      "epoch": 21.432,
      "grad_norm": 2.102583169937134,
      "learning_rate": 7.061484657783228e-05,
      "loss": 1.7554,
      "step": 1350
    },
    {
      "epoch": 21.592,
      "grad_norm": 2.1507551670074463,
      "learning_rate": 7.010880344162088e-05,
      "loss": 1.7266,
      "step": 1360
    },
    {
      "epoch": 21.752,
      "grad_norm": 2.0672500133514404,
      "learning_rate": 6.960029099661186e-05,
      "loss": 1.7445,
      "step": 1370
    },
    {
      "epoch": 21.912,
      "grad_norm": 2.272278070449829,
      "learning_rate": 6.908937168681176e-05,
      "loss": 1.7675,
      "step": 1380
    },
    {
      "epoch": 22.064,
      "grad_norm": 1.546988844871521,
      "learning_rate": 6.85761082517839e-05,
      "loss": 1.7399,
      "step": 1390
    },
    {
      "epoch": 22.224,
      "grad_norm": 1.9021533727645874,
      "learning_rate": 6.806056371894409e-05,
      "loss": 1.7092,
      "step": 1400
    },
    {
      "epoch": 22.384,
      "grad_norm": 1.8585351705551147,
      "learning_rate": 6.754280139582099e-05,
      "loss": 1.6974,
      "step": 1410
    },
    {
      "epoch": 22.544,
      "grad_norm": 1.8236180543899536,
      "learning_rate": 6.702288486228216e-05,
      "loss": 1.7632,
      "step": 1420
    },
    {
      "epoch": 22.704,
      "grad_norm": 2.4407851696014404,
      "learning_rate": 6.65008779627266e-05,
      "loss": 1.7375,
      "step": 1430
    },
    {
      "epoch": 22.864,
      "grad_norm": 2.1095447540283203,
      "learning_rate": 6.597684479824481e-05,
      "loss": 1.711,
      "step": 1440
    },
    {
      "epoch": 23.016,
      "grad_norm": 2.3151919841766357,
      "learning_rate": 6.545084971874738e-05,
      "loss": 1.7277,
      "step": 1450
    },
    {
      "epoch": 23.176,
      "grad_norm": 2.0688893795013428,
      "learning_rate": 6.492295731506296e-05,
      "loss": 1.7,
      "step": 1460
    },
    {
      "epoch": 23.336,
      "grad_norm": 2.0450408458709717,
      "learning_rate": 6.439323241100664e-05,
      "loss": 1.698,
      "step": 1470
    },
    {
      "epoch": 23.496,
      "grad_norm": 2.0957884788513184,
      "learning_rate": 6.386174005541986e-05,
      "loss": 1.7445,
      "step": 1480
    },
    {
      "epoch": 23.656,
      "grad_norm": 2.3259177207946777,
      "learning_rate": 6.332854551418247e-05,
      "loss": 1.6882,
      "step": 1490
    },
    {
      "epoch": 23.816,
      "grad_norm": 2.026665449142456,
      "learning_rate": 6.27937142621983e-05,
      "loss": 1.7182,
      "step": 1500
    },
    {
      "epoch": 23.976,
      "grad_norm": 2.00815749168396,
      "learning_rate": 6.2257311975355e-05,
      "loss": 1.7215,
      "step": 1510
    },
    {
      "epoch": 24.128,
      "grad_norm": 1.7704801559448242,
      "learning_rate": 6.171940452245924e-05,
      "loss": 1.7245,
      "step": 1520
    },
    {
      "epoch": 24.288,
      "grad_norm": 1.8449838161468506,
      "learning_rate": 6.118005795714813e-05,
      "loss": 1.7186,
      "step": 1530
    },
    {
      "epoch": 24.448,
      "grad_norm": 1.7047063112258911,
      "learning_rate": 6.063933850977811e-05,
      "loss": 1.708,
      "step": 1540
    },
    {
      "epoch": 24.608,
      "grad_norm": 2.0576789379119873,
      "learning_rate": 6.009731257929189e-05,
      "loss": 1.7041,
      "step": 1550
    },
    {
      "epoch": 24.768,
      "grad_norm": 2.2821035385131836,
      "learning_rate": 5.955404672506497e-05,
      "loss": 1.6919,
      "step": 1560
    },
    {
      "epoch": 24.928,
      "grad_norm": 2.1012675762176514,
      "learning_rate": 5.900960765873222e-05,
      "loss": 1.6238,
      "step": 1570
    },
    {
      "epoch": 25.08,
      "grad_norm": 1.971350908279419,
      "learning_rate": 5.846406223599597e-05,
      "loss": 1.6652,
      "step": 1580
    },
    {
      "epoch": 25.24,
      "grad_norm": 2.1154797077178955,
      "learning_rate": 5.7917477448416145e-05,
      "loss": 1.7128,
      "step": 1590
    },
    {
      "epoch": 25.4,
      "grad_norm": 2.202016830444336,
      "learning_rate": 5.7369920415184064e-05,
      "loss": 1.6666,
      "step": 1600
    },
    {
      "epoch": 25.56,
      "grad_norm": 2.0648844242095947,
      "learning_rate": 5.682145837488023e-05,
      "loss": 1.6572,
      "step": 1610
    },
    {
      "epoch": 25.72,
      "grad_norm": 1.8661748170852661,
      "learning_rate": 5.6272158677217666e-05,
      "loss": 1.6472,
      "step": 1620
    },
    {
      "epoch": 25.88,
      "grad_norm": 1.6232199668884277,
      "learning_rate": 5.57220887747716e-05,
      "loss": 1.7165,
      "step": 1630
    },
    {
      "epoch": 26.032,
      "grad_norm": 2.1963446140289307,
      "learning_rate": 5.5171316214696325e-05,
      "loss": 1.712,
      "step": 1640
    },
    {
      "epoch": 26.192,
      "grad_norm": 1.9337718486785889,
      "learning_rate": 5.46199086304307e-05,
      "loss": 1.6893,
      "step": 1650
    },
    {
      "epoch": 26.352,
      "grad_norm": 2.355964183807373,
      "learning_rate": 5.4067933733392915e-05,
      "loss": 1.6048,
      "step": 1660
    },
    {
      "epoch": 26.512,
      "grad_norm": 2.034938097000122,
      "learning_rate": 5.351545930466568e-05,
      "loss": 1.6859,
      "step": 1670
    },
    {
      "epoch": 26.672,
      "grad_norm": 2.1069464683532715,
      "learning_rate": 5.29625531866729e-05,
      "loss": 1.6362,
      "step": 1680
    },
    {
      "epoch": 26.832,
      "grad_norm": 2.6258084774017334,
      "learning_rate": 5.240928327484879e-05,
      "loss": 1.6945,
      "step": 1690
    },
    {
      "epoch": 26.992,
      "grad_norm": 1.7606053352355957,
      "learning_rate": 5.185571750930051e-05,
      "loss": 1.7051,
      "step": 1700
    },
    {
      "epoch": 27.144,
      "grad_norm": 2.313474655151367,
      "learning_rate": 5.130192386646533e-05,
      "loss": 1.6441,
      "step": 1710
    },
    {
      "epoch": 27.304,
      "grad_norm": 2.114698648452759,
      "learning_rate": 5.074797035076319e-05,
      "loss": 1.688,
      "step": 1720
    },
    {
      "epoch": 27.464,
      "grad_norm": 2.1396520137786865,
      "learning_rate": 5.0193924986246024e-05,
      "loss": 1.6437,
      "step": 1730
    },
    {
      "epoch": 27.624,
      "grad_norm": 1.87348210811615,
      "learning_rate": 4.9639855808244573e-05,
      "loss": 1.6414,
      "step": 1740
    },
    {
      "epoch": 27.784,
      "grad_norm": 2.1658527851104736,
      "learning_rate": 4.9085830855013826e-05,
      "loss": 1.7122,
      "step": 1750
    },
    {
      "epoch": 27.944,
      "grad_norm": 1.972912311553955,
      "learning_rate": 4.8531918159378056e-05,
      "loss": 1.6388,
      "step": 1760
    },
    {
      "epoch": 28.096,
      "grad_norm": 1.9190051555633545,
      "learning_rate": 4.797818574037659e-05,
      "loss": 1.6513,
      "step": 1770
    },
    {
      "epoch": 28.256,
      "grad_norm": 2.093958854675293,
      "learning_rate": 4.74247015949113e-05,
      "loss": 1.5656,
      "step": 1780
    },
    {
      "epoch": 28.416,
      "grad_norm": 2.1157314777374268,
      "learning_rate": 4.687153368939668e-05,
      "loss": 1.6429,
      "step": 1790
    },
    {
      "epoch": 28.576,
      "grad_norm": 1.9397295713424683,
      "learning_rate": 4.631874995141377e-05,
      "loss": 1.6497,
      "step": 1800
    },
    {
      "epoch": 28.736,
      "grad_norm": 2.556304693222046,
      "learning_rate": 4.576641826136884e-05,
      "loss": 1.6959,
      "step": 1810
    },
    {
      "epoch": 28.896,
      "grad_norm": 2.043426752090454,
      "learning_rate": 4.521460644415788e-05,
      "loss": 1.6711,
      "step": 1820
    },
    {
      "epoch": 29.048,
      "grad_norm": 2.1973648071289062,
      "learning_rate": 4.466338226083781e-05,
      "loss": 1.6212,
      "step": 1830
    },
    {
      "epoch": 29.208,
      "grad_norm": 2.9135706424713135,
      "learning_rate": 4.411281340030563e-05,
      "loss": 1.6313,
      "step": 1840
    },
    {
      "epoch": 29.368,
      "grad_norm": 2.0531437397003174,
      "learning_rate": 4.356296747098646e-05,
      "loss": 1.6869,
      "step": 1850
    },
    {
      "epoch": 29.528,
      "grad_norm": 2.3699357509613037,
      "learning_rate": 4.30139119925313e-05,
      "loss": 1.6214,
      "step": 1860
    },
    {
      "epoch": 29.688,
      "grad_norm": 2.457368850708008,
      "learning_rate": 4.246571438752585e-05,
      "loss": 1.6216,
      "step": 1870
    },
    {
      "epoch": 29.848,
      "grad_norm": 1.9757388830184937,
      "learning_rate": 4.1918441973211157e-05,
      "loss": 1.6422,
      "step": 1880
    },
    {
      "epoch": 30.0,
      "grad_norm": 3.4800078868865967,
      "learning_rate": 4.1372161953217306e-05,
      "loss": 1.6366,
      "step": 1890
    },
    {
      "epoch": 30.16,
      "grad_norm": 1.7737655639648438,
      "learning_rate": 4.082694140931089e-05,
      "loss": 1.6125,
      "step": 1900
    },
    {
      "epoch": 30.32,
      "grad_norm": 2.68974232673645,
      "learning_rate": 4.0282847293157624e-05,
      "loss": 1.6665,
      "step": 1910
    },
    {
      "epoch": 30.48,
      "grad_norm": 2.1723318099975586,
      "learning_rate": 3.973994641810079e-05,
      "loss": 1.6509,
      "step": 1920
    },
    {
      "epoch": 30.64,
      "grad_norm": 2.3354575634002686,
      "learning_rate": 3.919830545095681e-05,
      "loss": 1.588,
      "step": 1930
    },
    {
      "epoch": 30.8,
      "grad_norm": 1.9646881818771362,
      "learning_rate": 3.865799090382866e-05,
      "loss": 1.6297,
      "step": 1940
    },
    {
      "epoch": 30.96,
      "grad_norm": 2.1531591415405273,
      "learning_rate": 3.811906912593839e-05,
      "loss": 1.6401,
      "step": 1950
    },
    {
      "epoch": 31.112,
      "grad_norm": 2.7373440265655518,
      "learning_rate": 3.7581606295479606e-05,
      "loss": 1.6178,
      "step": 1960
    },
    {
      "epoch": 31.272,
      "grad_norm": 1.6840041875839233,
      "learning_rate": 3.704566841149095e-05,
      "loss": 1.5747,
      "step": 1970
    },
    {
      "epoch": 31.432,
      "grad_norm": 2.1207613945007324,
      "learning_rate": 3.651132128575164e-05,
      "loss": 1.6281,
      "step": 1980
    },
    {
      "epoch": 31.592,
      "grad_norm": 2.101768732070923,
      "learning_rate": 3.597863053469987e-05,
      "loss": 1.597,
      "step": 1990
    },
    {
      "epoch": 31.752,
      "grad_norm": 1.9108809232711792,
      "learning_rate": 3.544766157137535e-05,
      "loss": 1.6098,
      "step": 2000
    },
    {
      "epoch": 31.912,
      "grad_norm": 2.3348653316497803,
      "learning_rate": 3.491847959738673e-05,
      "loss": 1.6134,
      "step": 2010
    },
    {
      "epoch": 32.064,
      "grad_norm": 1.7913999557495117,
      "learning_rate": 3.4391149594905016e-05,
      "loss": 1.6128,
      "step": 2020
    },
    {
      "epoch": 32.224,
      "grad_norm": 2.0872931480407715,
      "learning_rate": 3.3865736318683924e-05,
      "loss": 1.5694,
      "step": 2030
    },
    {
      "epoch": 32.384,
      "grad_norm": 1.730960488319397,
      "learning_rate": 3.334230428810817e-05,
      "loss": 1.5616,
      "step": 2040
    },
    {
      "epoch": 32.544,
      "grad_norm": 2.649763822555542,
      "learning_rate": 3.282091777927065e-05,
      "loss": 1.5992,
      "step": 2050
    },
    {
      "epoch": 32.704,
      "grad_norm": 2.3284685611724854,
      "learning_rate": 3.23016408170795e-05,
      "loss": 1.6418,
      "step": 2060
    },
    {
      "epoch": 32.864,
      "grad_norm": 3.0655970573425293,
      "learning_rate": 3.178453716739602e-05,
      "loss": 1.6511,
      "step": 2070
    },
    {
      "epoch": 33.016,
      "grad_norm": 2.3443381786346436,
      "learning_rate": 3.12696703292044e-05,
      "loss": 1.6584,
      "step": 2080
    },
    {
      "epoch": 33.176,
      "grad_norm": 2.4128339290618896,
      "learning_rate": 3.075710352681416e-05,
      "loss": 1.5573,
      "step": 2090
    },
    {
      "epoch": 33.336,
      "grad_norm": 2.5795865058898926,
      "learning_rate": 3.0246899702096405e-05,
      "loss": 1.5724,
      "step": 2100
    },
    {
      "epoch": 33.496,
      "grad_norm": 2.441572904586792,
      "learning_rate": 2.9739121506754752e-05,
      "loss": 1.6108,
      "step": 2110
    },
    {
      "epoch": 33.656,
      "grad_norm": 2.2029240131378174,
      "learning_rate": 2.9233831294631808e-05,
      "loss": 1.6309,
      "step": 2120
    },
    {
      "epoch": 33.816,
      "grad_norm": 2.1051828861236572,
      "learning_rate": 2.873109111405231e-05,
      "loss": 1.6015,
      "step": 2130
    },
    {
      "epoch": 33.976,
      "grad_norm": 1.8668625354766846,
      "learning_rate": 2.8230962700203788e-05,
      "loss": 1.6359,
      "step": 2140
    },
    {
      "epoch": 34.128,
      "grad_norm": 2.6081221103668213,
      "learning_rate": 2.773350746755553e-05,
      "loss": 1.601,
      "step": 2150
    },
    {
      "epoch": 34.288,
      "grad_norm": 2.221876859664917,
      "learning_rate": 2.7238786502317208e-05,
      "loss": 1.5738,
      "step": 2160
    },
    {
      "epoch": 34.448,
      "grad_norm": 2.057393789291382,
      "learning_rate": 2.674686055493748e-05,
      "loss": 1.5765,
      "step": 2170
    },
    {
      "epoch": 34.608,
      "grad_norm": 2.1494977474212646,
      "learning_rate": 2.6257790032644125e-05,
      "loss": 1.6221,
      "step": 2180
    },
    {
      "epoch": 34.768,
      "grad_norm": 2.273406744003296,
      "learning_rate": 2.5771634992026162e-05,
      "loss": 1.6404,
      "step": 2190
    },
    {
      "epoch": 34.928,
      "grad_norm": 2.0025806427001953,
      "learning_rate": 2.528845513165896e-05,
      "loss": 1.5518,
      "step": 2200
    },
    {
      "epoch": 35.08,
      "grad_norm": 2.2410154342651367,
      "learning_rate": 2.480830978477358e-05,
      "loss": 1.5882,
      "step": 2210
    },
    {
      "epoch": 35.24,
      "grad_norm": 2.692775249481201,
      "learning_rate": 2.4331257911970628e-05,
      "loss": 1.5772,
      "step": 2220
    },
    {
      "epoch": 35.4,
      "grad_norm": 2.3754918575286865,
      "learning_rate": 2.3857358093980214e-05,
      "loss": 1.6069,
      "step": 2230
    },
    {
      "epoch": 35.56,
      "grad_norm": 2.2023367881774902,
      "learning_rate": 2.3386668524468232e-05,
      "loss": 1.5856,
      "step": 2240
    },
    {
      "epoch": 35.72,
      "grad_norm": 2.412209987640381,
      "learning_rate": 2.291924700289047e-05,
      "loss": 1.6342,
      "step": 2250
    },
    {
      "epoch": 35.88,
      "grad_norm": 2.5998127460479736,
      "learning_rate": 2.245515092739488e-05,
      "loss": 1.51,
      "step": 2260
    },
    {
      "epoch": 36.032,
      "grad_norm": 2.2470154762268066,
      "learning_rate": 2.19944372877733e-05,
      "loss": 1.5993,
      "step": 2270
    },
    {
      "epoch": 36.192,
      "grad_norm": 2.4428083896636963,
      "learning_rate": 2.153716265846325e-05,
      "loss": 1.6111,
      "step": 2280
    },
    {
      "epoch": 36.352,
      "grad_norm": 2.3593475818634033,
      "learning_rate": 2.1083383191600674e-05,
      "loss": 1.5723,
      "step": 2290
    },
    {
      "epoch": 36.512,
      "grad_norm": 2.256287097930908,
      "learning_rate": 2.0633154610124705e-05,
      "loss": 1.5426,
      "step": 2300
    },
    {
      "epoch": 36.672,
      "grad_norm": 1.8422645330429077,
      "learning_rate": 2.0186532200934926e-05,
      "loss": 1.5752,
      "step": 2310
    },
    {
      "epoch": 36.832,
      "grad_norm": 2.5258686542510986,
      "learning_rate": 1.97435708081024e-05,
      "loss": 1.5603,
      "step": 2320
    },
    {
      "epoch": 36.992,
      "grad_norm": 2.542649984359741,
      "learning_rate": 1.930432482613482e-05,
      "loss": 1.6191,
      "step": 2330
    },
    {
      "epoch": 37.144,
      "grad_norm": 2.148942232131958,
      "learning_rate": 1.886884819329715e-05,
      "loss": 1.6052,
      "step": 2340
    },
    {
      "epoch": 37.304,
      "grad_norm": 2.15437912940979,
      "learning_rate": 1.843719438498806e-05,
      "loss": 1.5979,
      "step": 2350
    },
    {
      "epoch": 37.464,
      "grad_norm": 1.9746112823486328,
      "learning_rate": 1.800941640717326e-05,
      "loss": 1.5399,
      "step": 2360
    },
    {
      "epoch": 37.624,
      "grad_norm": 2.5525712966918945,
      "learning_rate": 1.7585566789876618e-05,
      "loss": 1.6247,
      "step": 2370
    },
    {
      "epoch": 37.784,
      "grad_norm": 1.882749080657959,
      "learning_rate": 1.7165697580729477e-05,
      "loss": 1.5498,
      "step": 2380
    },
    {
      "epoch": 37.944,
      "grad_norm": 2.994635820388794,
      "learning_rate": 1.674986033857947e-05,
      "loss": 1.5902,
      "step": 2390
    },
    {
      "epoch": 38.096,
      "grad_norm": 2.197725772857666,
      "learning_rate": 1.633810612715908e-05,
      "loss": 1.6304,
      "step": 2400
    },
    {
      "epoch": 38.256,
      "grad_norm": 2.313232421875,
      "learning_rate": 1.5930485508815302e-05,
      "loss": 1.5713,
      "step": 2410
    },
    {
      "epoch": 38.416,
      "grad_norm": 2.411879777908325,
      "learning_rate": 1.5527048538300633e-05,
      "loss": 1.5356,
      "step": 2420
    },
    {
      "epoch": 38.576,
      "grad_norm": 2.2809836864471436,
      "learning_rate": 1.5127844756626435e-05,
      "loss": 1.5428,
      "step": 2430
    },
    {
      "epoch": 38.736,
      "grad_norm": 2.1830852031707764,
      "learning_rate": 1.4732923184979563e-05,
      "loss": 1.6066,
      "step": 2440
    },
    {
      "epoch": 38.896,
      "grad_norm": 2.340202569961548,
      "learning_rate": 1.4342332318702539e-05,
      "loss": 1.5794,
      "step": 2450
    },
    {
      "epoch": 39.048,
      "grad_norm": 2.5116982460021973,
      "learning_rate": 1.3956120121338579e-05,
      "loss": 1.5638,
      "step": 2460
    },
    {
      "epoch": 39.208,
      "grad_norm": 2.3749678134918213,
      "learning_rate": 1.3574334018741686e-05,
      "loss": 1.5845,
      "step": 2470
    },
    {
      "epoch": 39.368,
      "grad_norm": 2.427556037902832,
      "learning_rate": 1.3197020893252964e-05,
      "loss": 1.5963,
      "step": 2480
    },
    {
      "epoch": 39.528,
      "grad_norm": 2.387827157974243,
      "learning_rate": 1.2824227077943562e-05,
      "loss": 1.5589,
      "step": 2490
    },
    {
      "epoch": 39.688,
      "grad_norm": 2.6316895484924316,
      "learning_rate": 1.245599835092504e-05,
      "loss": 1.5619,
      "step": 2500
    },
    {
      "epoch": 39.848,
      "grad_norm": 2.408318042755127,
      "learning_rate": 1.2092379929728038e-05,
      "loss": 1.5582,
      "step": 2510
    },
    {
      "epoch": 40.0,
      "grad_norm": 3.434882164001465,
      "learning_rate": 1.1733416465749559e-05,
      "loss": 1.5891,
      "step": 2520
    },
    {
      "epoch": 40.16,
      "grad_norm": 2.009803056716919,
      "learning_rate": 1.1379152038770036e-05,
      "loss": 1.5607,
      "step": 2530
    },
    {
      "epoch": 40.32,
      "grad_norm": 1.8879936933517456,
      "learning_rate": 1.1029630151540282e-05,
      "loss": 1.5493,
      "step": 2540
    },
    {
      "epoch": 40.48,
      "grad_norm": 2.542963981628418,
      "learning_rate": 1.0684893724439654e-05,
      "loss": 1.5697,
      "step": 2550
    },
    {
      "epoch": 40.64,
      "grad_norm": 2.5944197177886963,
      "learning_rate": 1.0344985090205345e-05,
      "loss": 1.5855,
      "step": 2560
    },
    {
      "epoch": 40.8,
      "grad_norm": 2.261159896850586,
      "learning_rate": 1.0009945988734204e-05,
      "loss": 1.5416,
      "step": 2570
    },
    {
      "epoch": 40.96,
      "grad_norm": 2.825915813446045,
      "learning_rate": 9.679817561957094e-06,
      "loss": 1.613,
      "step": 2580
    },
    {
      "epoch": 41.112,
      "grad_norm": 2.3008275032043457,
      "learning_rate": 9.354640348786764e-06,
      "loss": 1.5205,
      "step": 2590
    },
    {
      "epoch": 41.272,
      "grad_norm": 2.921428680419922,
      "learning_rate": 9.034454280139837e-06,
      "loss": 1.5331,
      "step": 2600
    },
    {
      "epoch": 41.432,
      "grad_norm": 2.146044969558716,
      "learning_rate": 8.719298674033328e-06,
      "loss": 1.6069,
      "step": 2610
    },
    {
      "epoch": 41.592,
      "grad_norm": 2.6462693214416504,
      "learning_rate": 8.409212230756564e-06,
      "loss": 1.5097,
      "step": 2620
    },
    {
      "epoch": 41.752,
      "grad_norm": 1.8091522455215454,
      "learning_rate": 8.104233028118802e-06,
      "loss": 1.579,
      "step": 2630
    },
    {
      "epoch": 41.912,
      "grad_norm": 2.322888135910034,
      "learning_rate": 7.804398516773465e-06,
      "loss": 1.6128,
      "step": 2640
    },
    {
      "epoch": 42.064,
      "grad_norm": 2.5771727561950684,
      "learning_rate": 7.50974551561926e-06,
      "loss": 1.6158,
      "step": 2650
    },
    {
      "epoch": 42.224,
      "grad_norm": 2.0829381942749023,
      "learning_rate": 7.220310207278863e-06,
      "loss": 1.5644,
      "step": 2660
    },
    {
      "epoch": 42.384,
      "grad_norm": 2.1490209102630615,
      "learning_rate": 6.936128133655889e-06,
      "loss": 1.5689,
      "step": 2670
    },
    {
      "epoch": 42.544,
      "grad_norm": 2.306628942489624,
      "learning_rate": 6.657234191570344e-06,
      "loss": 1.5228,
      "step": 2680
    },
    {
      "epoch": 42.704,
      "grad_norm": 2.7662603855133057,
      "learning_rate": 6.3836626284734465e-06,
      "loss": 1.5748,
      "step": 2690
    },
    {
      "epoch": 42.864,
      "grad_norm": 2.616649627685547,
      "learning_rate": 6.115447038242094e-06,
      "loss": 1.5685,
      "step": 2700
    },
    {
      "epoch": 43.016,
      "grad_norm": 2.2485554218292236,
      "learning_rate": 5.852620357053651e-06,
      "loss": 1.565,
      "step": 2710
    },
    {
      "epoch": 43.176,
      "grad_norm": 2.2091856002807617,
      "learning_rate": 5.5952148593414636e-06,
      "loss": 1.5664,
      "step": 2720
    },
    {
      "epoch": 43.336,
      "grad_norm": 2.6901235580444336,
      "learning_rate": 5.343262153831596e-06,
      "loss": 1.5121,
      "step": 2730
    },
    {
      "epoch": 43.496,
      "grad_norm": 2.4600279331207275,
      "learning_rate": 5.096793179661463e-06,
      "loss": 1.5543,
      "step": 2740
    },
    {
      "epoch": 43.656,
      "grad_norm": 2.3618483543395996,
      "learning_rate": 4.855838202580476e-06,
      "loss": 1.5588,
      "step": 2750
    },
    {
      "epoch": 43.816,
      "grad_norm": 2.290353775024414,
      "learning_rate": 4.620426811233558e-06,
      "loss": 1.5864,
      "step": 2760
    },
    {
      "epoch": 43.976,
      "grad_norm": 2.1245880126953125,
      "learning_rate": 4.390587913527694e-06,
      "loss": 1.5877,
      "step": 2770
    },
    {
      "epoch": 44.128,
      "grad_norm": 2.298379421234131,
      "learning_rate": 4.166349733082153e-06,
      "loss": 1.5549,
      "step": 2780
    },
    {
      "epoch": 44.288,
      "grad_norm": 2.0542807579040527,
      "learning_rate": 3.94773980576269e-06,
      "loss": 1.5551,
      "step": 2790
    },
    {
      "epoch": 44.448,
      "grad_norm": 2.6515066623687744,
      "learning_rate": 3.734784976300165e-06,
      "loss": 1.572,
      "step": 2800
    },
    {
      "epoch": 44.608,
      "grad_norm": 2.05426287651062,
      "learning_rate": 3.5275113949941598e-06,
      "loss": 1.5567,
      "step": 2810
    },
    {
      "epoch": 44.768,
      "grad_norm": 2.1290526390075684,
      "learning_rate": 3.325944514501711e-06,
      "loss": 1.5536,
      "step": 2820
    },
    {
      "epoch": 44.928,
      "grad_norm": 2.574495315551758,
      "learning_rate": 3.13010908671188e-06,
      "loss": 1.5546,
      "step": 2830
    },
    {
      "epoch": 45.08,
      "grad_norm": 2.161444664001465,
      "learning_rate": 2.940029159706165e-06,
      "loss": 1.5827,
      "step": 2840
    },
    {
      "epoch": 45.24,
      "grad_norm": 2.02801775932312,
      "learning_rate": 2.755728074805597e-06,
      "loss": 1.5301,
      "step": 2850
    },
    {
      "epoch": 45.4,
      "grad_norm": 1.9003499746322632,
      "learning_rate": 2.5772284637043563e-06,
      "loss": 1.6045,
      "step": 2860
    },
    {
      "epoch": 45.56,
      "grad_norm": 2.659280776977539,
      "learning_rate": 2.404552245690761e-06,
      "loss": 1.5123,
      "step": 2870
    },
    {
      "epoch": 45.72,
      "grad_norm": 2.327200174331665,
      "learning_rate": 2.2377206249555803e-06,
      "loss": 1.5415,
      "step": 2880
    },
    {
      "epoch": 45.88,
      "grad_norm": 2.2486813068389893,
      "learning_rate": 2.076754087988214e-06,
      "loss": 1.5653,
      "step": 2890
    },
    {
      "epoch": 46.032,
      "grad_norm": 2.074096441268921,
      "learning_rate": 1.9216724010610477e-06,
      "loss": 1.5725,
      "step": 2900
    },
    {
      "epoch": 46.192,
      "grad_norm": 2.433713436126709,
      "learning_rate": 1.7871461841391601e-06,
      "loss": 1.5344,
      "step": 2910
    },
    {
      "epoch": 46.352,
      "grad_norm": 2.2077932357788086,
      "learning_rate": 1.6432975783323146e-06,
      "loss": 1.5138,
      "step": 2920
    },
    {
      "epoch": 46.512,
      "grad_norm": 2.3378727436065674,
      "learning_rate": 1.505387049898449e-06,
      "loss": 1.5701,
      "step": 2930
    },
    {
      "epoch": 46.672,
      "grad_norm": 2.528435230255127,
      "learning_rate": 1.3734315338920057e-06,
      "loss": 1.5938,
      "step": 2940
    },
    {
      "epoch": 46.832,
      "grad_norm": 2.2610907554626465,
      "learning_rate": 1.247447234107385e-06,
      "loss": 1.5959,
      "step": 2950
    },
    {
      "epoch": 46.992,
      "grad_norm": 1.9429244995117188,
      "learning_rate": 1.127449621089155e-06,
      "loss": 1.5477,
      "step": 2960
    },
    {
      "epoch": 47.144,
      "grad_norm": 2.126420259475708,
      "learning_rate": 1.013453430232303e-06,
      "loss": 1.5609,
      "step": 2970
    },
    {
      "epoch": 47.304,
      "grad_norm": 2.3910903930664062,
      "learning_rate": 9.054726599727836e-07,
      "loss": 1.5501,
      "step": 2980
    },
    {
      "epoch": 47.464,
      "grad_norm": 2.7125771045684814,
      "learning_rate": 8.035205700685167e-07,
      "loss": 1.5684,
      "step": 2990
    },
    {
      "epoch": 47.624,
      "grad_norm": 1.8637869358062744,
      "learning_rate": 7.076096799711773e-07,
      "loss": 1.5244,
      "step": 3000
    },
    {
      "epoch": 47.784,
      "grad_norm": 2.2944834232330322,
      "learning_rate": 6.177517672887712e-07,
      "loss": 1.5641,
      "step": 3010
    },
    {
      "epoch": 47.944,
      "grad_norm": 1.9590115547180176,
      "learning_rate": 5.339578663394295e-07,
      "loss": 1.5744,
      "step": 3020
    },
    {
      "epoch": 48.096,
      "grad_norm": 2.629405975341797,
      "learning_rate": 4.5623826679641047e-07,
      "loss": 1.5581,
      "step": 3030
    },
    {
      "epoch": 48.256,
      "grad_norm": 2.195469856262207,
      "learning_rate": 3.846025124245145e-07,
      "loss": 1.5575,
      "step": 3040
    },
    {
      "epoch": 48.416,
      "grad_norm": 2.2695274353027344,
      "learning_rate": 3.1905939990821143e-07,
      "loss": 1.5513,
      "step": 3050
    },
    {
      "epoch": 48.576,
      "grad_norm": 2.3995988368988037,
      "learning_rate": 2.5961697777134307e-07,
      "loss": 1.5998,
      "step": 3060
    },
    {
      "epoch": 48.736,
      "grad_norm": 1.8594324588775635,
      "learning_rate": 2.0628254538886948e-07,
      "loss": 1.4955,
      "step": 3070
    },
    {
      "epoch": 48.896,
      "grad_norm": 1.9491093158721924,
      "learning_rate": 1.5906265209045257e-07,
      "loss": 1.5614,
      "step": 3080
    },
    {
      "epoch": 49.048,
      "grad_norm": 2.357384204864502,
      "learning_rate": 1.1796309635626612e-07,
      "loss": 1.534,
      "step": 3090
    },
    {
      "epoch": 49.208,
      "grad_norm": 2.1769590377807617,
      "learning_rate": 8.298892510493205e-08,
      "loss": 1.5797,
      "step": 3100
    },
    {
      "epoch": 49.368,
      "grad_norm": 2.2919270992279053,
      "learning_rate": 5.4144433073771707e-08,
      "loss": 1.5489,
      "step": 3110
    },
    {
      "epoch": 49.528,
      "grad_norm": 2.3223118782043457,
      "learning_rate": 3.143316229144433e-08,
      "loss": 1.545,
      "step": 3120
    },
    {
      "epoch": 49.688,
      "grad_norm": 2.460470199584961,
      "learning_rate": 1.4857901642967298e-08,
      "loss": 1.5918,
      "step": 3130
    },
    {
      "epoch": 49.848,
      "grad_norm": 2.382174253463745,
      "learning_rate": 4.420686527273343e-09,
      "loss": 1.5702,
      "step": 3140
    },
    {
      "epoch": 50.0,
      "grad_norm": 3.415544271469116,
      "learning_rate": 1.2279860724384585e-10,
      "loss": 1.5295,
      "step": 3150
    }
  ],
  "logging_steps": 10,
  "max_steps": 3150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.69581692928e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
