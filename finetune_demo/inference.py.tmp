import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

def load_model(base_model_path, adapter_path):
    # 4-bit quantization configuration
    compute_dtype = torch.float16
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
        llm_int8_enable_fp32_cpu_offload=True
    )

    # Custom device map for memory management
    max_memory = {0: "4500MB", "cpu": "24GB"}
    
    # Load base model
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        device_map="auto",
        max_memory=max_memory,
        trust_remote_code=True,
        offload_folder="offload"
    )
    
    # Load LoRA adapter
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()  # Set to evaluation mode
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def format_prompt(instruction, input_text=None):
    if input_text:
        return f"Instruction: {instruction}\nInput: {input_text}\nOutput:"
    return f"Instruction: {instruction}\nOutput:"

def generate_response(model, tokenizer, instruction, input_text=None, max_length=512):
    # Format the prompt like in training
    prompt = format_prompt(instruction, input_text)
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    max_input_length = len(inputs.input_ids[0])
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_input_length + 256,  # Limit response length
            min_length=max_input_length + 10,   # Ensure some meaningful response
            num_return_sequences=1,
            temperature=0.8,           # Slightly higher temperature
            do_sample=True,
            top_p=0.9,                # Nucleus sampling
            top_k=50,                 # Top-k sampling
            repetition_penalty=1.2,    # Penalize repetitions
            no_repeat_ngram_size=3,    # Prevent 3-gram repetitions
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            length_penalty=1.0
        )
    
    response = tokenizer.decode(outputs[0][max_input_length:], skip_special_tokens=True)
    
    # Clean up the response
    response = response.strip()
    # Remove repeated lines
    lines = response.split('\n')
    unique_lines = []
    for line in lines:
        if line.strip() and line not in unique_lines:
            unique_lines.append(line)
    response = '\n'.join(unique_lines)
    
    return response

def main():
    # Load model and adapter
    base_model = "facebook/opt-350m"  # Same as training
    adapter_path = "outputs"  # Path to your trained adapter
    
    print("Loading model...")
    model, tokenizer = load_model(base_model, adapter_path)
    
    # Test prompts - simpler cases first
    test_cases = [
        {
            "instruction": "What is 2+2?",
            "input": None
        },
        {
            "instruction": "Complete this sentence: The cat is...",
            "input": None
        },
        {
            "instruction": "Translate to French",
            "input": "Good morning"
        }
    ]
    
    print("\nGenerating responses...")
    for test in test_cases:
        print("\n" + "="*50)
        print(f"Instruction: {test['instruction']}")
        if test['input']:
            print(f"Input: {test['input']}")
        response = generate_response(model, tokenizer, test['instruction'], test['input'])
        print(f"Output: {response}")
        print("="*50)

if __name__ == "__main__":
    main()
